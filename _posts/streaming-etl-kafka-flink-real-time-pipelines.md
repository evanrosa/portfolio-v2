---
title: "Why Streaming ETL Is the Future — and How to Get Started"
coverImage: "/assets/blog/streaming-is-the-future/cover.png"
date: "2025-04-03"
excerpt: "Streaming ETL is no longer a niche—it's the foundation of real-time, event-driven systems. In this post, I break down when to use streaming pipelines, how Kafka and Flink fit together, and walk through a real-world example."
tags:
  [
    "streaming ETL",
    "Kafka Flink pipeline",
    "real-time data engineering",
    "data pipeline examples",
    "data engineering",
  ]
author: "Evan Rosa"
ogImage:
  url: "/assets/blog/streaming-is-the-future/cover.png"
---

> Data doesn’t wait. Neither should your pipelines. That’s why streaming ETL is becoming the default for modern, responsive data architectures.

## ⏱️ Why Real-Time Data Matters

In a world of instant decisions—fraud detection, product recommendations, live dashboards—**batch processing just isn’t fast enough**.

Modern businesses demand:

- **Real-time visibility**
- **Continuous insights**
- **Immediate reactions to events**

Enter **streaming ETL**—a shift from hourly or nightly jobs to systems that process data _as it happens_.

## 🔁 What Is Streaming ETL?

Streaming ETL refers to continuously:

1. **Extracting** data from real-time sources (like APIs, event logs, databases)
2. **Transforming** it on the fly (filtering, enriching, aggregating)
3. **Loading** it into destinations (data lakes, warehouses, dashboards)

This often happens **in milliseconds**, not minutes or hours.

> Think: Kafka → Flink → Iceberg → BigQuery/Superset.

## 🧠 When Do You Need Streaming?

Streaming ETL is ideal when:

- You track **live events** (clicks, transactions, IoT signals)
- You need **low-latency alerts** (fraud detection, outage detection)
- Your stakeholders want **real-time dashboards**
- You’re building **event-driven architectures** (microservices, data mesh)

✅ **Example Use Cases**:

- Real-time churn alerts in SaaS platforms
- Live tracking of user engagement across mobile apps
- Financial transaction monitoring
- Sports analytics dashboards (e.g. live match predictions)

## ⚙️ Kafka + Flink: A Stream Dream Team

Here’s how they fit together:

| Component             | Role                                                |
| --------------------- | --------------------------------------------------- |
| **Kafka**             | Message broker that ingests and delivers data       |
| **Flink**             | Real-time processing engine that transforms streams |
| **Iceberg**           | Stores transformed data with schema/version control |
| **Airflow**           | Orchestrates hybrid (batch + streaming) workflows   |
| **BigQuery/Superset** | Destination for analytics and reporting             |

### Kafka → Flink Example Flow

1. Kafka topic receives JSON from an API (e.g. user signup events)
2. Flink job:
   - Parses schema
   - Enriches with user metadata
   - Deduplicates based on event IDs
   - Aggregates by region or funnel step
3. Data lands in Iceberg tables
4. Optional batch roll-ups in BigQuery
5. Superset dashboard updates in real-time

> In one of my projects, this pipeline supported synthetic subscription events that mimicked user behavior—processed in real time with Flink and visualized via Superset.

## ⚖️ Trade-offs: Streaming vs Batch

| Dimension      | Batch                          | Streaming                              |
| -------------- | ------------------------------ | -------------------------------------- |
| **Latency**    | Minutes to hours               | Seconds to milliseconds                |
| **Cost**       | Cheaper at small scale         | Higher infra cost, but scalable        |
| **Complexity** | Easier to debug and monitor    | Needs careful state and fault handling |
| **Use Cases**  | BI reports, historical rollups | Real-time alerts, dashboards, triggers |
| **Tools**      | Airflow, Spark, SQLMesh        | Kafka, Flink, Spark Streaming          |

**My take?** Start streaming-first. Then add batch rollups for cost-effective aggregates.

## 🧱 Getting Started: My Streaming Stack

Here’s the stack I use for most of my modern streaming projects:

- **Kafka** (event ingestion + partitioning)
- **Flink** (real-time transformations)
- **Iceberg** (versioned, schema-evolving tables)
- **Airflow** (orchestrate hybrid workflows)
- **BigQuery** (long-term storage + SQL analytics)
- **Superset** (dashboards and insights)

All containerized with **Docker**, designed to scale locally or in the cloud.

💡 Tip: Build a small prototype first. Kafka + Flink + Superset is a powerful learning stack.

## 🔄 Batch + Streaming = Hybrid

Batch and streaming don’t have to compete.

In my work, I often:

- Ingest **real-time data via Kafka**
- Process **live streams with Flink**
- Store **raw + enriched data in Iceberg**
- Schedule **batch rollups with SQLMesh or Airflow**
- Serve **dashboards from BigQuery or Superset**

This hybrid approach gives you **the best of both worlds**.

## 📢 Up Next

Next post: _“How I’d Build a Modern Data Stack From Scratch in 2025”_  
Get my unfiltered take on tools, trade-offs, and how I’d architect a flexible, future-proof platform.

---

**🔗 Connect with me:**  
[LinkedIn](https://www.linkedin.com/in/evan-rosa/) | [Portfolio](https://www.evro.dev/)
